\documentclass[a4paper, 10pt, titlepage]{article}

\usepackage[
    a4paper,
    lmargin=25.4mm,
    rmargin=25.4mm,
    tmargin=20mm,
    bmargin=20mm
]{geometry}

\usepackage[ddmmyyyy]{datetime}
\usepackage[export]{adjustbox}
\usepackage[hidelinks]{hyperref}
\usepackage{array}
\usepackage[numbib,nottoc]{tocbibind}
\usepackage{caption}
\usepackage{cite}
\usepackage{color}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{listing}
\usepackage{longtable}
\usepackage{multicol}
\usepackage{nameref}
\usepackage{parskip}
\usepackage{pgffor}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{url}

\IfFileExists{inconsolata.sty}{\usepackage{inconsolata}}

\newcommand{\code}[1]{\small\texttt{#1}\normalsize}

\definecolor{codegray}{gray}{0.9}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{inputpath={{../src/}}}
\lstdefinestyle{numbers} {numbers=left, numberstyle=\ttfamily}
\lstdefinestyle{color}
{
    commentstyle=\color{dkgreen},
    keywordstyle=\color{blue},
    stringstyle=\color{mauve},
}

\lstdefinestyle{common}
{
    breakatwhitespace=false,
    breaklines=true,
    columns=fixed,
    showstringspaces=false,
    xleftmargin=0.65cm,
    basicstyle=\footnotesize\ttfamily,
    tabsize=4,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
    literate={*}{*\allowbreak}1,
    numbersep=10pt,
}

\lstdefinestyle{code} {style=common, style=color, style=numbers}
\lstdefinestyle{raw} {style=common, style=color}

\fancyhf{}
\setlength\columnseprule{0.4pt}
\setlength{\parindent}{0pt}

\graphicspath{{../src/}}

\captionsetup{
    width=.85\linewidth,
    justification=centering
}

\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\title{\huge \textbf{Machine Perception Assignment\\
Digits Extraction And Recognition}}
\author{Julian Heng (19473701)}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\pagestyle{fancy}

\fancyhf[HL]{\footnotesize{
    Machine Perception Assignment - Digits Extraction And Recognition
}}
\fancyhf[FC]{\thepage}
\fancyhf[FL]{\footnotesize{Julian Heng (19473701)}}

\section{Discussion}
\fancyhead[HR]{\footnotesize{Discussion}}

\subsection{Attempts}

In this assignment, I've managed to train on the provided digits to create a
digits classifier, extracted digits from the provided digits and successfully
managed to predict the correct digits for each extracted area.

I've started this assignment after completing Sam's OCR challenge, where digit
recognition was performed with contours and KNN. I've decided to reuse certain
components from that practical into this assignment. I've only used contours
since starting this assignment and did not experiment with other methods of
extracting digits using Harris corners or SIFT because I think that using
contours provides very good results.

Most of the time spent on the assignment was extracting the digits from the
images. The hardest challenge when extracting the digits was trying to isolate
only the contours that contains the numbers without any contours containing
noise. I've tried filtering by contour features and have managed to just filter
only just the relevant contours, but it brought along the issue that the
filters were too specific to the provided images and will fail on other images.
Therefore, further ideas needs to be explored and will be discussed in the
section \nameref{impl-classifier}.

I've also used KNN to detect the digits because I've already completed the
aforementioned practical where KNN was already fully implemented.


\subsection{Implementation Details}

\subsubsection{Training Implementation}
\label{impl-train}

The implementation of the digits training utilises the OpenCV
\code{ml\_KNearest} object class to train and detect digits. The folder
structure of the provided images makes it so that it is easy to create the
labels for each of the training images. Each of the images needs to be
preprocessed before it is trained upon.

The preprocessing performs an Otsu binary threshold (\code{cv2.threshold})
before connected components is applied. Connected components is used instead of
using the unmodified binary image to remove any possible noise from the
threshold. Therefore, we assume that the digit will be the largest component.
Hence, we extract the component with the largest area.

\foreach \x in {9-7,3-11}
{
    \begin{figure}[H]
        \centering{
            \fbox{\includegraphics[]{{DEBUG_training_orig_digit\x}.jpg}}
            \fbox{\includegraphics[]{{DEBUG_training_bin_digit\x}.jpg}}
            \fbox{\includegraphics[]{{DEBUG_training_digit\x}.jpg}}
        }
        \caption{The preprocessing of the training image `digit\x.jpg'}
    \end{figure}
}

This extract is then flattened into a single dimension array to be trained by
the KNN. This is repeated throughout all of the images provided to the program
and outputs the training data and training labels as a Numpy array export file.
This file is then loaded when the program is running under classifying mode in
order to predict the digits.


\subsubsection{Classifier Implementation}
\label{impl-classifier}

\begin{samepage}
There are a total 6 steps to extract and classify digits in the program's
implementation:

\begin{enumerate}
    \item Process the image using a Gaussian filter and Otsu binary
        threshold
    \item Extract the contours for the digits
    \item Crop the image to the contours
    \item Process the cropped image using a Gaussian filter and Otsu binary
        threshold
    \item Perform connected components on the crop
    \item Detect and predict the digit on each connected component
\end{enumerate}
\end{samepage}


\paragraph{Image Preprocessing}
\label{impl-img-preprocessing}

OpenCV's Gaussian filter (\code{cv2.GaussianFilter}) is applied to remove noise
from the images. The reduction of noise in the image will overall improve the
Otsu binary thresholding (\code{cv2.threshold}) results when applied. The
Gaussian Filter kernel is 5 as any kernel size large than 7 will cause some of
the digits in the provided images to distort after binary threshold is applied,
thus causing the contours extraction to not be able to extract the full digit.

Otsu thresholding is the prefered method because Otsu thresholding allows using
a threshold value that minimises the weighted within-class variance. Therefore,
the threshold value depends on the images and the distribution of the
intensities of the different pixels within the image.


\begin{figure}[H]
    \centering
    \fbox{\includegraphics[]{{DEBUG_bin_tr06}.jpg}}
    \caption{An example image after preprocessing is applied}
\end{figure}


\paragraph{Contours Extraction}

Using the preprocessed image from the previous step, the contours are then
extracted (\code{cv2.findContours}). Once all the contours are extracted, all
of the contours containing noise needs to be filtered.

\begin{samepage}
In order to extract the contours containing the digits, the program implements
these 3 stages:

\begin{enumerate}
    \item Filter contours depending on their features, such as width, height
        and area
    \item Group contours by distance
    \item Group contours by angles
\end{enumerate}
\end{samepage}

\begin{samepage}
The first stage finds the bounding box (\code{cv2.boundingRect}) and the area
(\code{cv2.contourArea}) of the contour. Using these features, we can check
that:

\begin{enumerate}
    \item The contour's height is more than the width
    \item The contour's aspect ratio is within the threshold
    \item The contour's height to image height ratio is not too large
    \item The contour's width to image width ratio is not too large
    \item The contour's area is not less than 70 pixels
    \item The contour's area to image area is within the threshold
    \item The contour is not along the edges of the image
\end{enumerate}
\end{samepage}

If any one of these conditions fails, then the contour is filtered.

The second stage involves grouping the contours by their shortest distances
between other contours. The main idea behind this stage is that the digits are
closer to other digits than other contours containing noise. Hence, all of the
relevant digit contours will be part of the same group, with all the contours
containing noise belonging to a different group.

In order to achieve this stage, for each possible contour pairing in the list
of filtered contours, if the shortest distance is within $\frac{1}{12}$ of the
image's hypotenuse, then the contour pairing will be grouped together. If
either contours belongs to another group, then the groups will be conjoined.

The third stage is very similar to the second stage. Instead of grouping
contours by distance, the contours will be grouped by the angle of the line
between their centers. The main idea behind this stage is that the digits are
along a line. By grouping contours that fall within a straight horizontal line,
noisy contours not within the same line as the digits will be separate.

The issue with this idea is that it works well if the plate containing the
digits are perpendicular to the camera. If the plates containing the digits are
taken at an angle, then the digits may be grouped into different groups. This,
however, separate any contours within the groups that are directly below the
digit contours.

After all 3 stages passes, the remaining contours will be the digits and, if
any, a small amount of noisy contours. Out of the possible contour groups,
the program will select the largest contour groups, under the assumption that
the contour group containing the digits has the largest area. This is to ensure
that the correct contour group is selected, but would only limit to detecting
only one set of digits per image.

The inspiration for filtering the contours by size and features comes from this
StackOverflow answer\cite{impl-1}. Within the post, it is suggested to ``erase
contours with very small area'' as well as ``erase contours with low aspect
ratio''. The first stage checks if the contours are within the minimum size
allowed, as well as the ratio is what we expect the digits to be around, along
with filtering any contours close to the edges of the image.

The idea for the second and third stages, as well as the grouping contours, are
ideas that I've come up with and implemented fully.

\begin{figure}[H]
    \centering
    \fbox{\includegraphics[]{{DEBUG_contours_tr06}.jpg}}
    \caption{The contour groups extracted from the image}
\end{figure}

\begin{figure}[H]
    \centering{
        \fbox{\includegraphics[]{{DEBUG_cropped_0_tr06}.jpg}}
        \fbox{\includegraphics[]{{DEBUG_cropped_1_tr06}.jpg}}
    }
    \caption{The individually cropped contour groups from the image}
\end{figure}

Out of the three stages, the first stage is the filter that removes the most
amount of contours from the list of found contours. This is beneficial for
stage two and three because both those stages have a time complexity of
$O(n^2)$, where as stage one has a time complexity of $O(n)$. Since stage one
filters contours before stage two and stage three, the running time for both
these stages decreases exponentially as more contours are removed. Therefore,
stage one is rather aggressive when filtering invalid the contours.

This aggressive nature brings forwards issues where the filtering is
overfitting the provided images given to extract the digits. Overfitting in a
way where it works best on the provided images, but works less optimally on
other images of similar types. Under extended testing of extra images of house
numbers searched on the internet, it is observed that it is able to extract
some digits, fail to extract others and included some noise along with the
extracts.


\paragraph{Digit Recognition}

Using the largest contour group, the image is cropped to contain the largest
contour. The same image preprocessing from \nameref{impl-img-preprocessing}
will be applied. Connected components is then used to get each individual
digits of the binary image.

Since Otsu binary thresholding is applied again on the cropped image, it is
possible that the resulting binary threshold image is different to the binary
threshold image from the image preprocessing when extracting the contours. This
can result in better binary images as the histograms differ between the cropped
image and the original image. Hence, we preprocess the image twice to get
better results overall when extracting the digits for prediction.

\begin{figure}[H]
    \centering{
        \fbox{\adjincludegraphics[
            trim={{0.25\width} {0.66\height} {0.25\width} {0.1\height}},
            width=0.4\linewidth,
            keepaspectratio,
            clip,
        ]{{DEBUG_bin_val04}.jpg}}
        \fbox{\includegraphics[
            width=0.4\linewidth,
            keepaspectratio,
        ]{{DEBUG_cropped_bin_val04}.jpg}}
    }
    \caption{
        The preprocessed image of `val04.jpg' during the extraction of the
        contours and the classification of the cropped. The right image is
        cropped for visibility. Note the difference in the second digit.
    }
\end{figure}

For each individual component mask, detect if there are any pixels that are on
the edge of the component. If there are, then a single pixel black border is
applied on the edge containing that pixel. Then, more additional padding will
be applied to the top and bottom of the component mask such that the mask will
have the same aspect ratio as the training images (\code{cv2.copyMakeBorder}).

Finally, the mask is resized to the training image's size (\code{cv2.resize}).
Using the KNN created in section \nameref{impl-train}, \code{findNearest} with
$k = 1$ is used to predict what the digit is.

Connected components is used to extract the individual digits because connected
components was also used when training the classifier, and is also easy to
implement the extraction of the mask. Using contours would yield the same
result, but would require more steps to prepare the contours for prediction.

For example, consider a contour group containing the digit 0. The digit 0
contains 2 contours, the digit 0 and the hole within the digit 0. The inner
contour needs to be removed in order to correctly draw the mask containing the
digit 0. Determining if a contour is within another contour has a time
complexity of $O(n^2)$. Hence, it is much rather prefered to use conntected
components to extract the digit masks for prediction.

\begin{figure}[H]
    \centering{
        \fbox{\includegraphics[]{{DEBUG_component_0_tr06}.jpg}}
        \fbox{\includegraphics[]{{DEBUG_component_1_tr06}.jpg}}
    }
    \caption{
        The connected components of the digits from the cropped image used to
        predict
    }
\end{figure}


\subsection{Results}

\begin{longtable}{
    | >{\centering\arraybackslash}m{0.02\columnwidth}
    | >{\centering\arraybackslash}m{0.28\columnwidth}
    | >{\centering\arraybackslash}m{0.28\columnwidth}
    | >{\centering\arraybackslash}m{0.28\columnwidth} |
}
    \hline & DetectedAreaXX.jpg & BoundingBoxXX.txt & HouseXX.txt \\
    \hline 01 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea01}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox01.txt} &
        \lstinputlisting[style=raw]{House01.txt} \\
    \hline 02 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea02}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox02.txt} &
        \lstinputlisting[style=raw]{House02.txt} \\
    \hline 03 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea03}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox03.txt} &
        \lstinputlisting[style=raw]{House03.txt} \\
    \hline 04 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea04}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox04.txt} &
        \lstinputlisting[style=raw]{House04.txt} \\
    \hline 05 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea05}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox05.txt} &
        \lstinputlisting[style=raw]{House05.txt} \\
    \hline 06 & \includegraphics[
        width=\linewidth,
        height=0.1\textheight,
        keepaspectratio,
    ]{{DetectedArea06}.jpg} &
        \lstinputlisting[style=raw]{BoundingBox06.txt} &
        \lstinputlisting[style=raw]{House06.txt} \\
    \hline
    \caption{
        The results of the validation files
    }
\end{longtable}

From these results, we can conclude that the program works well with the
provided validation images. It is able to correctly extract the digits, as well
as correctly classify each digit. It is important to note, however, that while
this program is able correctly predict these images, it is not guaranteed to be
able to extract and detect digits with 100\% accuracy.


\bibliographystyle{plainurl}
\bibliography{report.bib}


\newpage
\section{Source Code}

\subsection{colors.py}

\fancyhead[HR]{\footnotesize{Source Code - colors.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/colors.py}
\newpage


\subsection{image.py}

\fancyhead[HR]{\footnotesize{Source Code - image.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/image.py}
\newpage


\subsection{\_\_init\_\_.py}

\fancyhead[HR]{\footnotesize{Source Code - \_\_init\_\_.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/__init__.py}
\newpage


\subsection{\_\_main\_\_.py}

\fancyhead[HR]{\footnotesize{Source Code - \_\_main\_\_.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/__main__.py}
\newpage


\subsection{mp\_ocr.py}

\fancyhead[HR]{\footnotesize{Source Code - mp\_ocr.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/mp_ocr.py}
\newpage


\subsection{ocr.py}

\fancyhead[HR]{\footnotesize{Source Code - ocr.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/ocr.py}
\newpage


\subsection{train/\_\_init\_\_.py}

\fancyhead[HR]{\footnotesize{Source Code - train/\_\_init\_\_.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/train/__init__.py}
\newpage


\subsection{train/knn.py}

\fancyhead[HR]{\footnotesize{Source Code - train/knn.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/train/knn.py}
\newpage


\subsection{utils.py}

\fancyhead[HR]{\footnotesize{Source Code - utils.py}}
\lstinputlisting[language=Python,style=code]{mp_ocr/utils.py}
\newpage

\end{document}
